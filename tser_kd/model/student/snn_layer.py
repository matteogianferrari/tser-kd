import torch
import torch.nn as nn
import snntorch as snn


class LayerTWrapper(nn.Module):
    """Wraps a spatial layer to apply it independently along the temporal dimension.

    Attributes:
        layer: The spatial layer to execute at each time step.
        batch_norm: Batch normalization layer that expects input of shape [B, C, T, H, W].
    """

    def __init__(self, layer: nn.Module, batch_norm: nn.Module = None) -> None:
        """Initializes the LayerTWrapper.

        The batch normalization doesn't work for linear layers.

        Args:
            layer: A PyTorch layer that processes a single time slice of shape [B, C, H, W] or [B, N].
            batch_norm: A batch normalization layer that can accept a tensor of shape [T, B, C, H, W]
                and perform normalization across time and spatial dimensions.
        """
        super(LayerTWrapper, self).__init__()

        # Layers attributes
        self.layer = layer
        self.batch_norm = batch_norm

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies the wrapped layer across the time dimension and then optional batch normalization.

        This approach vectorizes the time dimension and allow the layer to process all the time steps in one pass.

        Args:
            x: Input tensor of shape [T, B, C, H, W] or [T, B, N].

        Returns:
            torch.Tensor: Output tensor of shape [T, B, C, H, W] or [T, B, N] normalized along
                the temporal and spatial dimensions.
        """
        # Retrieves the time steps and batch-size of the input tensor
        # 'in_spatial' could be [C, H, W] or [N]
        T, B, *in_spatial = x.shape

        # Collapses the time and batch dimensions
        x = x.reshape(T * B, *in_spatial)

        # One forward pass for the layer
        x = self.layer(x)

        # Retrieves the new spatial dimension
        _, *out_spatial = x.shape
        x = x.reshape(T, B, *out_spatial)

        # Applies batch normalization if present
        if self.batch_norm is not None:
            # Changes the order of dimensions in the tensor
            x = x.permute(1, 2, 0, 3, 4).contiguous()

            # Applies batch normalization
            x = self.batch_norm(x)

            # Changes the order of dimensions in the tensor
            x = x.permute(2, 0, 1, 3, 4).contiguous()

        return x


class LIFTWrapper(nn.Module):
    """Layer that wraps snnTorch Leaky layer to allow it to integrate over input spikes.

    Attributes:
        layer: An snnTorch Leaky neuron layer that performs membrane potential integration
            and spike generation per time step.
    """

    def __init__(self, layer: snn.Leaky) -> None:
        """Initializes the LIFTWrapper.

        Args:
            layer: A preconfigured snnTorch Leaky neuron layer.
        """
        super(LIFTWrapper, self).__init__()

        self.layer = layer

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Applies the Leaky layer over the time dimension of the input spike train.

        Iterates through T time steps, feeding each slice x[t] of shape [B, C, H, W] into the Leaky
        layer. Collects the output spikes at each step, stacks them into a tensor of shape
        [T, B, C, H, W], and then resets the Leaky layer's hidden and membrane states.

        Args:
            x: Input spike train tensor of shape [T, B, C, H, W].

        Returns:
            torch.Tensor: Output spike tensor of shape [T, B, C, H, W], containing the spikes
                generated by the Leaky layer at each time step.
        """
        # List of outputs spikes
        x_out = []

        # Retrieves the number of time steps, x.shape: [T, B, C, H, W]
        T = x.size(0)

        # Forward pass in time
        for t in range(T):
            # Retrieves the sample at time step t, x_t.shape: [B, C, H, W]
            x_t = x[t]

            # Appends the layer output
            x_out.append(self.layer(x_t))

        # Converts the list into a tensor
        x_out = torch.stack(x_out)

        # Resets the membrane potential and the hidden state to avoid backprop errors
        self.layer.reset_hidden()
        self.layer.reset_mem()

        return x_out
